{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction** means predicting the most likely word or phrase that will come next in a sentence or text. It is like having an inbuilt feature on an application that suggests the next word as you type or speak. The Next Word Prediction Models are used in applications like messaging apps, search engines, virtual assistants, and autocorrect features on smartphones."
      ],
      "metadata": {
        "id": "vUZevTFXFKmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses: The Next Word Prediction models have a range of applications across various industries. For example, when you start typing a message on your phone, it suggests the next word to speed up your typing. Similarly, search engines predict and show search suggestions as you type in the search bar. Next word prediction helps us communicate faster and more accurately by anticipating what we might say or search for."
      ],
      "metadata": {
        "id": "VJJU0Q18FS5J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qJczcU6ZHhyZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Read the text file\n",
        "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tokenize the text to create a sequence of words:"
      ],
      "metadata": {
        "id": "hr7pklUxFZwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yTb1XbEXH1LR"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the text is tokenized, which means it is divided into individual words or tokens. The ‘Tokenizer’ object is created, which will handle the tokenization process. The ‘fit_on_texts’ method of the tokenizer is called, passing the ‘text’ as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index. The ‘total_words’ variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text."
      ],
      "metadata": {
        "id": "4taRPzLnFdWr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uyVkcCpfH2rQ"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the text data is split into lines using the ‘\\n’ character as a delimiter. For each line in the text, the ‘texts_to_sequences’ method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary. The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index ‘i’."
      ],
      "metadata": {
        "id": "N4oE_BJ2F9K9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the ‘input_sequences’ list. This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model."
      ],
      "metadata": {
        "id": "HWQVUiIWGArw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iaMJVLfsH7TD"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the input sequences are padded to ensure all sequences have the same length. The variable ‘max_sequence_len’ is assigned the maximum length among all the input sequences. The ‘pad_sequences’ function is used to pad or truncate the input sequences to match this maximum length.\n",
        "\n",
        "The ‘pad_sequences’ function takes the input_sequences list, sets the maximum length to ‘max_sequence_len’, and specifies that the padding should be added at the beginning of each sequence using the ‘padding=pre’ argument. Finally, the input sequences are converted into a numpy array to facilitate further processing.\n",
        "\n",
        "Now let’s split the sequences into input and output:"
      ],
      "metadata": {
        "id": "ahzFL_SUGD6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OZumutREH_lC"
      },
      "outputs": [],
      "source": [
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, the ‘y’ array is assigned the values of the last column in the ‘input_sequences’ array, which represents the target or predicted word.\n",
        "\n",
        "Now let’s convert the output to one-hot encode vectors:"
      ],
      "metadata": {
        "id": "JvwnnnN_GKnP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wAj1s_4vIDvc"
      },
      "outputs": [],
      "source": [
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector.\n",
        "\n",
        "Now let’s build a neural network architecture to train the model:"
      ],
      "metadata": {
        "id": "WxlLW0nuGOWZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8tqFPYIIJGh",
        "outputId": "c22d4883-2aa4-428d-d4a2-ae1bd70766c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 17, 100)           820000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8200)              1238200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2208800 (8.43 MB)\n",
            "Trainable params: 2208800 (8.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above defines the model architecture for the next word prediction model. The ‘Sequential’ model is created, which represents a linear stack of layers. The first layer added to the model is the ‘Embedding’ layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n",
        "\n",
        "‘total_words’, which represents the total number of distinct words in the vocabulary;\n",
        "‘100’, which denotes the dimensionality of the word embeddings;\n",
        "and ‘input_length’, which specifies the length of the input sequences.\n",
        "The next layer added is the ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
        "\n",
        "Finally, the ‘Dense’ layer is added, which is a fully connected layer that produces the output predictions. It has ‘total_words’ units and uses the ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence."
      ],
      "metadata": {
        "id": "rLTqbZH1GTsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhc980YzIOnG",
        "outputId": "24a1f4a5-09db-45bc-b6c6-977de1a213dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3010/3010 [==============================] - 97s 31ms/step - loss: 6.2359 - accuracy: 0.0770\n",
            "Epoch 2/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 5.5182 - accuracy: 0.1236\n",
            "Epoch 3/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 5.1305 - accuracy: 0.1463\n",
            "Epoch 4/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 4.7993 - accuracy: 0.1647\n",
            "Epoch 5/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 4.4935 - accuracy: 0.1831\n",
            "Epoch 6/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 4.2054 - accuracy: 0.2038\n",
            "Epoch 7/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 3.9294 - accuracy: 0.2299\n",
            "Epoch 8/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 3.6696 - accuracy: 0.2603\n",
            "Epoch 9/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 3.4262 - accuracy: 0.2930\n",
            "Epoch 10/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 3.1988 - accuracy: 0.3278\n",
            "Epoch 11/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 2.9897 - accuracy: 0.3631\n",
            "Epoch 12/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 2.7984 - accuracy: 0.3969\n",
            "Epoch 13/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 2.6188 - accuracy: 0.4301\n",
            "Epoch 14/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 2.4559 - accuracy: 0.4627\n",
            "Epoch 15/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 2.3048 - accuracy: 0.4918\n",
            "Epoch 16/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 2.1675 - accuracy: 0.5215\n",
            "Epoch 17/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 2.0394 - accuracy: 0.5478\n",
            "Epoch 18/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.9240 - accuracy: 0.5698\n",
            "Epoch 19/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.8175 - accuracy: 0.5929\n",
            "Epoch 20/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.7171 - accuracy: 0.6160\n",
            "Epoch 21/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.6276 - accuracy: 0.6334\n",
            "Epoch 22/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.5456 - accuracy: 0.6527\n",
            "Epoch 23/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.4677 - accuracy: 0.6681\n",
            "Epoch 24/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.3980 - accuracy: 0.6846\n",
            "Epoch 25/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.3344 - accuracy: 0.6976\n",
            "Epoch 26/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.2750 - accuracy: 0.7118\n",
            "Epoch 27/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.2200 - accuracy: 0.7224\n",
            "Epoch 28/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.1684 - accuracy: 0.7351\n",
            "Epoch 29/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 1.1239 - accuracy: 0.7428\n",
            "Epoch 30/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.0809 - accuracy: 0.7539\n",
            "Epoch 31/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 1.0420 - accuracy: 0.7622\n",
            "Epoch 32/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 1.0035 - accuracy: 0.7714\n",
            "Epoch 33/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 0.9701 - accuracy: 0.7775\n",
            "Epoch 34/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 0.9375 - accuracy: 0.7852\n",
            "Epoch 35/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 0.9094 - accuracy: 0.7904\n",
            "Epoch 36/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.8829 - accuracy: 0.7959\n",
            "Epoch 37/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.8553 - accuracy: 0.8029\n",
            "Epoch 38/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.8316 - accuracy: 0.8069\n",
            "Epoch 39/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.8117 - accuracy: 0.8130\n",
            "Epoch 40/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.7919 - accuracy: 0.8157\n",
            "Epoch 41/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.7739 - accuracy: 0.8190\n",
            "Epoch 42/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.7544 - accuracy: 0.8224\n",
            "Epoch 43/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.7411 - accuracy: 0.8266\n",
            "Epoch 44/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.7247 - accuracy: 0.8305\n",
            "Epoch 45/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.7086 - accuracy: 0.8336\n",
            "Epoch 46/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6993 - accuracy: 0.8348\n",
            "Epoch 47/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.6850 - accuracy: 0.8390\n",
            "Epoch 48/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6782 - accuracy: 0.8387\n",
            "Epoch 49/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6620 - accuracy: 0.8432\n",
            "Epoch 50/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.6566 - accuracy: 0.8438\n",
            "Epoch 51/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 0.6456 - accuracy: 0.8458\n",
            "Epoch 52/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.6366 - accuracy: 0.8482\n",
            "Epoch 53/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.6296 - accuracy: 0.8483\n",
            "Epoch 54/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.6216 - accuracy: 0.8509\n",
            "Epoch 55/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.6106 - accuracy: 0.8530\n",
            "Epoch 56/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.6100 - accuracy: 0.8517\n",
            "Epoch 57/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.6012 - accuracy: 0.8541\n",
            "Epoch 58/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5902 - accuracy: 0.8563\n",
            "Epoch 59/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5896 - accuracy: 0.8564\n",
            "Epoch 60/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5885 - accuracy: 0.8559\n",
            "Epoch 61/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5815 - accuracy: 0.8572\n",
            "Epoch 62/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5724 - accuracy: 0.8604\n",
            "Epoch 63/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5779 - accuracy: 0.8567\n",
            "Epoch 64/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5678 - accuracy: 0.8591\n",
            "Epoch 65/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5649 - accuracy: 0.8597\n",
            "Epoch 66/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5584 - accuracy: 0.8609\n",
            "Epoch 67/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5529 - accuracy: 0.8638\n",
            "Epoch 68/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5574 - accuracy: 0.8615\n",
            "Epoch 69/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5478 - accuracy: 0.8636\n",
            "Epoch 70/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5523 - accuracy: 0.8618\n",
            "Epoch 71/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.5423 - accuracy: 0.8646\n",
            "Epoch 72/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5463 - accuracy: 0.8628\n",
            "Epoch 73/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.5405 - accuracy: 0.8640\n",
            "Epoch 74/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.5357 - accuracy: 0.8650\n",
            "Epoch 75/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5345 - accuracy: 0.8650\n",
            "Epoch 76/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5321 - accuracy: 0.8649\n",
            "Epoch 77/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5301 - accuracy: 0.8660\n",
            "Epoch 78/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5324 - accuracy: 0.8652\n",
            "Epoch 79/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5268 - accuracy: 0.8664\n",
            "Epoch 80/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 0.5250 - accuracy: 0.8661\n",
            "Epoch 81/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5275 - accuracy: 0.8648\n",
            "Epoch 82/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 0.5227 - accuracy: 0.8661\n",
            "Epoch 83/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5152 - accuracy: 0.8685\n",
            "Epoch 84/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5236 - accuracy: 0.8659\n",
            "Epoch 85/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5201 - accuracy: 0.8658\n",
            "Epoch 86/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5132 - accuracy: 0.8690\n",
            "Epoch 87/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5173 - accuracy: 0.8667\n",
            "Epoch 88/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5201 - accuracy: 0.8667\n",
            "Epoch 89/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5171 - accuracy: 0.8675\n",
            "Epoch 90/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5152 - accuracy: 0.8663\n",
            "Epoch 91/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5104 - accuracy: 0.8679\n",
            "Epoch 92/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5093 - accuracy: 0.8685\n",
            "Epoch 93/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5139 - accuracy: 0.8663\n",
            "Epoch 94/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5101 - accuracy: 0.8670\n",
            "Epoch 95/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5032 - accuracy: 0.8695\n",
            "Epoch 96/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5027 - accuracy: 0.8692\n",
            "Epoch 97/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5048 - accuracy: 0.8684\n",
            "Epoch 98/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5034 - accuracy: 0.8697\n",
            "Epoch 99/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5032 - accuracy: 0.8687\n",
            "Epoch 100/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5047 - accuracy: 0.8679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7da74058cd60>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the model is being compiled and trained. The ‘compile’ method configures the model for training. The ‘loss’ parameter is set to ‘categorical_crossentropy’, a commonly used loss function for multi-class classification problems. The ‘optimizer’ parameter is set to ‘adam’, an optimization algorithm that adapts the learning rate during training.\n",
        "\n",
        "\n",
        "The ‘metrics’ parameter is set to ‘accuracy’ to monitor the accuracy during training. After compiling the model, the ‘fit’ method is called to train the model on the input sequences ‘X’ and the corresponding output ‘y’. The ‘epochs’ parameter specifies the number of times the training process will iterate over the entire dataset. The ‘verbose’ parameter is set to ‘1’ to display the training process.\n",
        "\n",
        "The above code will take more than an hour to execute. Once the code is executed, here’s how we can generate the next word predictions using our model:"
      ],
      "metadata": {
        "id": "egTS7Z0wGZO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I will leave if they\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz9k4YowCWe0",
        "outputId": "61fb657f-d798-4ed2-cf2e-54d2783136cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 447ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "I will leave if they were good days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I will leave if they\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQPy5VFECdnl",
        "outputId": "d4d16c4a-77ca-4482-f959-69a229f58dc7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "I will leave if they were good days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"The Man with\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO8e_0ZjCjeN",
        "outputId": "f74e6372-2b9c-4097-850b-fcea0a68dd94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "The Man with the twisted lip\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}